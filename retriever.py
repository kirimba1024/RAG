from typing import List
from pathlib import Path

import torch
from elasticsearch import Elasticsearch
from llama_index.core.base.base_retriever import BaseRetriever
from llama_index.core.schema import QueryBundle, BaseNode, TextNode, NodeWithScore
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank

from utils import ES_URL, ES_INDEX, EMBED_MODEL, RERANK_MODEL, setup_logging

logger = setup_logging(Path(__file__).stem)

ES = Elasticsearch(ES_URL, request_timeout=30, max_retries=3, retry_on_timeout=True)

Settings.embed_model = HuggingFaceEmbedding(EMBED_MODEL, normalize=True)

DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
RERANKER = SentenceTransformerRerank(model=RERANK_MODEL, top_n=10, device=DEVICE)

def normal_prefix(id_prefix):
    return (id_prefix or "").lstrip("/").lstrip(".")


class HybridESRetriever(BaseRetriever):
    def __init__(self, es, index, path_prefix: str, top_k=20):
        super().__init__()
        self.es = es
        self.index = index
        self.top_k = top_k
        self.path_prefix = normal_prefix(path_prefix)

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        query_embedding = Settings.embed_model.get_text_embedding(query_bundle.query_str)
        filters = []
        if self.path_prefix:
            filters.append({"prefix": {"doc_id": self.path_prefix}})
        body = {
            "size": self.top_k,
            "knn": {
                "field": "embedding",
                "query_vector": query_embedding,
                "k": self.top_k,
                "num_candidates": self.top_k * 5,
            },
            "query": {
                "bool": {
                    "should": [
                        {
                            "multi_match": {
                                "query": query_bundle.query_str,
                                "fields": ["text^1.0", "text.ru^1.3", "text.en^1.2"],
                            }
                        }
                    ]
                }
            }
        }
        if filters:
            body["query"]["bool"]["filter"] = filters
            knn_filter = {"bool": {"must": filters}}
            body["knn"]["filter"] = knn_filter
        response = self.es.search(
            index=self.index,
            knn=body["knn"],
            query=body["query"],
            size=body["size"],
            request_timeout=30
        )
        nodes = []
        for hit in response["hits"]["hits"]:
            source = hit["_source"]
            metadata = source.get("metadata", {}).copy()
            metadata["doc_id"] = source.get("doc_id")
            node = TextNode(id_=hit["_id"], text=source["text"], metadata=metadata)
            nodes.append(NodeWithScore(node=node, score=float(hit["_score"])))
        return nodes


def retrieve_fusion_nodes(question: str, path_prefix: str, top_n: int) -> List[BaseNode]:
    retriever = HybridESRetriever(es=ES, index=ES_INDEX, path_prefix=path_prefix, top_k=top_n * 3)
    candidates = retriever.retrieve(question)
    logger.info(f"üîç Retriever –≤–µ—Ä–Ω—É–ª {len(candidates)} —á–∞–Ω–∫–æ–≤ (query: '{question[:50]}...')")
    qb = QueryBundle(query_str=question)
    RERANKER.top_n = top_n
    reranked = RERANKER.postprocess_nodes(candidates, query_bundle=qb)
    logger.info(f"‚≠ê Reranker –æ—Ç–æ–±—Ä–∞–ª {len(reranked)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(candidates)}")
    return [nws.node for nws in reranked]


def get_code_stats(path_prefix: str = "") -> str:
    """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã"""
    query_filter = {"prefix": {"doc_id": path_prefix}} if path_prefix else {"match_all": {}}
    query = {
        "size": 0,
        "query": query_filter,
        "aggs": {
            "files": {"cardinality": {"field": "doc_id.keyword"}},
            "chunks": {"value_count": {"field": "_id"}},
            "languages": {"terms": {"field": "language.keyword", "size": 10}},
            "top_files": {
                "terms": {"field": "doc_id.keyword", "size": 10},
                "aggs": {"chunk_count": {"value_count": {"field": "metadata.chunk_id"}}}
            },
            "avg_chunk_size": {"avg": {"field": "metadata.end_line"}},
            "largest_files": {
                "terms": {"field": "doc_id.keyword", "size": 5},
                "aggs": {"max_lines": {"max": {"field": "metadata.end_line"}}}
            },
            "recent_files": {
                "terms": {"field": "doc_id.keyword", "size": 5},
            },
            "file_extensions": {
                "terms": {"field": "file_extension.keyword", "size": 10}
            }
        }
    }
    response = ES.search(index=ES_INDEX, body=query)
    aggs = response["aggregations"]
    results = [f"üìä –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞" + (f" ({path_prefix})" if path_prefix else "")]
    results.extend([
        f"üìÅ –§–∞–π–ª–æ–≤: {aggs['files']['value']}",
        f"üìÑ –ß–∞–Ω–∫–æ–≤: {aggs['chunks']['value']}",
        f"üìè –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {aggs['avg_chunk_size']['value'] or 0:.0f} —Å—Ç—Ä–æ–∫",
        "",
        "üåê –Ø–∑—ã–∫–∏:"
    ])
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –≤ —Ü–∏–∫–ª–µ
    sections = [
        ("üåê –Ø–∑—ã–∫–∏:", aggs["languages"]["buckets"], lambda x: f"  {x['key']}: {x['doc_count']}"),
        ("üìà –¢–æ–ø —Ñ–∞–π–ª–æ–≤ –ø–æ —á–∞–Ω–∫–∞–º:", aggs["top_files"]["buckets"], lambda x: f"  {x['key']}: {x['chunk_count']['value']} —á–∞–Ω–∫–æ–≤"),
        ("üìä –°–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã:", aggs["largest_files"]["buckets"], lambda x: f"  {x['key']}: {x['max_lines']['value']} —Å—Ç—Ä–æ–∫"),
        ("üìÅ –ü–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º:", aggs["file_extensions"]["buckets"], lambda x: f"  .{x['key']}: {x['doc_count']} —Ñ–∞–π–ª–æ–≤")
    ]
    
    for title, items, formatter in sections:
        results.extend(["", title])
        for item in items:
            results.append(formatter(item))
    
    return "\n".join(results)


def get_architecture_stats(path_prefix: str = "") -> str:
    """–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã"""
    query_filter = {"prefix": {"doc_id": path_prefix}} if path_prefix else {"match_all": {}}
    query = {
        "size": 0,
        "query": query_filter,
        "aggs": {
            "complexity_stats": {"stats": {"field": "complexity_score"}},
            "test_coverage": {"terms": {"field": "is_test_file", "size": 2}},
            "documentation_ratio": {"terms": {"field": "has_documentation", "size": 2}},
            "architecture_layers": {"terms": {"field": "layer.keyword", "size": 10}},
            "dependency_density": {"avg": {"field": "dependency_count"}},
            "code_duplication": {"terms": {"field": "is_duplicate", "size": 2}}
        }
    }
    response = ES.search(index=ES_INDEX, body=query)
    aggs = response["aggregations"]
    
    results = [f"üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞" + (f" ({path_prefix})" if path_prefix else "")]
    
    results.extend(["", "üßÆ –°–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–¥–∞:"])
    complexity = aggs["complexity_stats"]
    results.append(f"  –°—Ä–µ–¥–Ω—è—è: {complexity['avg'] or 0:.1f}")
    results.append(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {complexity['max'] or 0:.1f}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –≤ —Ü–∏–∫–ª–µ
    sections = [
        ("üß™ –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏:", aggs["test_coverage"]["buckets"], lambda x: f"  {'–¢–µ—Å—Ç—ã' if x['key'] else '–û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥'}: {x['doc_count']} —Ñ–∞–π–ª–æ–≤"),
        ("üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:", aggs["documentation_ratio"]["buckets"], lambda x: f"  {'–° –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π' if x['key'] else '–ë–µ–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏'}: {x['doc_count']} —Ñ–∞–π–ª–æ–≤"),
        ("üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Å–ª–æ–∏:", aggs["architecture_layers"]["buckets"], lambda x: f"  {x['key']}: {x['doc_count']} —Ñ–∞–π–ª–æ–≤"),
        ("üîÑ –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞:", aggs["code_duplication"]["buckets"], lambda x: f"  {'–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π' if x['key'] else '–£–Ω–∏–∫–∞–ª—å–Ω—ã–π'}: {x['doc_count']} —Ñ–∞–π–ª–æ–≤")
    ]
    
    for title, items, formatter in sections:
        results.extend(["", title])
        for item in items:
            results.append(formatter(item))
    
    results.extend(["", "üîó –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:"])
    results.append(f"  –°—Ä–µ–¥–Ω—è—è: {aggs['dependency_density']['value'] or 0:.1f} –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –Ω–∞ —Ñ–∞–π–ª")
    
    return "\n".join(results)
